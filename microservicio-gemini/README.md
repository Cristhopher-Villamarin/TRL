# Microservicio de AnÃ¡lisis TRL con Google Cloud Gemini

Microservicio FastAPI para evaluar el nivel de madurez tecnolÃ³gica (TRL) de documentos PDF usando Google Cloud Vertex AI (Gemini 2.0 Pro).

## ğŸ“‹ DescripciÃ³n

Este microservicio recibe archivos PDF de documentos tecnolÃ³gicos, los procesa con el modelo Gemini 2.0 Pro de Google Cloud, y genera un informe completo de anÃ¡lisis TRL (Technology Readiness Level) que incluye:
- EvaluaciÃ³n de cada nivel TRL (1 al 9)
- Evidencias tÃ©cnicas encontradas
- Puntajes por nivel
- TRL global alcanzado
- Recomendaciones para avanzar al siguiente nivel

## ğŸš€ CaracterÃ­sticas

- âœ… API REST con FastAPI
- âœ… EvaluaciÃ³n TRL con Google Cloud Gemini 2.0 Pro
- âœ… Basado en matrices oficiales TRL (ESPE)
- âœ… Docker y Docker Compose
- âœ… AutenticaciÃ³n con Google Cloud ADC (Application Default Credentials)
- âœ… CORS configurado
- âœ… ValidaciÃ³n de datos con Pydantic
- âœ… Logging estructurado
- âœ… Health check endpoint

## ğŸ“ Estructura del Proyecto

```
microservicio-gemini/
â”œâ”€â”€ .env                      # Variables de entorno
â”œâ”€â”€ .env.example              # Ejemplo de configuraciÃ³n
â”œâ”€â”€ .gitignore                # Archivos ignorados por Git
â”œâ”€â”€ .dockerignore             # Archivos ignorados por Docker
â”œâ”€â”€ requirements.txt          # Dependencias Python
â”œâ”€â”€ Dockerfile                # ConfiguraciÃ³n Docker
â”œâ”€â”€ docker-compose.yml        # OrquestaciÃ³n de contenedores
â”œâ”€â”€ README.md                 # Este archivo
â”œâ”€â”€ main.py                   # AplicaciÃ³n FastAPI
â”œâ”€â”€ config.py                 # ConfiguraciÃ³n centralizada
â”œâ”€â”€ datTRL/                   # Matrices TRL oficiales (ESPE)
â”‚   â”œâ”€â”€ matriz2.txt           # Evidencias por nivel TRL
â”‚   â”œâ”€â”€ matriz3.txt           # Puntajes mÃ­nimos por TRL
â”‚   â””â”€â”€ matriz4.txt           # Rangos para TRL global
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ schemas.py            # Modelos Pydantic
â””â”€â”€ services/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ analizador_TRL.py     # LÃ³gica de anÃ¡lisis TRL con Gemini
```

## ğŸ”§ Requisitos Previos

1. **Python 3.11+** (para desarrollo local sin Docker)
2. **Docker y Docker Compose** (para containerizaciÃ³n)
3. **Google Cloud SDK** instalado y configurado
4. **Credenciales de Google Cloud** configuradas

### Configurar Google Cloud Credentials

```bash
# Instalar gcloud CLI
# https://cloud.google.com/sdk/docs/install

# Autenticarse
gcloud auth login

# Configurar Application Default Credentials (ADC)
gcloud auth application-default login

# Configurar proyecto
gcloud config set project tesis-475602
```

## ğŸƒ Inicio RÃ¡pido

### OpciÃ³n 1: Con Docker Compose (Recomendado)

```bash
# 1. Clonar/ubicarse en el directorio
cd microservicio-gemini

# 2. Copiar y configurar variables de entorno
cp .env.example .env
# Editar .env con tus valores

# 3. Construir y ejecutar
docker-compose up --build

# El servicio estarÃ¡ disponible en http://localhost:5000
```

### OpciÃ³n 2: Local sin Docker

```bash
# 1. Crear entorno virtual
python -m venv venv

# 2. Activar entorno virtual
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Copiar y configurar .env
cp .env.example .env

# 5. Ejecutar
python main.py
# O con uvicorn:
uvicorn main:app --host 0.0.0.0 --port 5000 --reload
```

## ğŸ“¡ Endpoints

### 1. Root
```
GET /
```
Retorna informaciÃ³n bÃ¡sica del servicio.

### 2. Health Check
```
GET /health
```
Verifica que el servicio estÃ© funcionando y muestra configuraciÃ³n.

**Respuesta:**
```json
{
  "status": "healthy",
  "project_id": "tesis-475602",
  "model_id": "gemini-2.0-flash"
}
```

### 3. Analizar Documento TecnolÃ³gico
```
POST /analizar
Content-Type: multipart/form-data
```

**ParÃ¡metros:**
- `file`: Archivo PDF del documento tecnolÃ³gico (multipart/form-data)

**Respuesta exitosa:**
```json
{
  "success": true,
  "table_text": "=== ANÃLISIS TRL ===\n\nTRL ALCANZADO: TRL 5\n\nEVIDENCIAS:\n- TRL 1: Cumplido (85/100)\n- TRL 2: Cumplido (75/100)\n...",
  "error": null
}
```

**Respuesta con error:**
```json
{
  "success": false,
  "table_text": null,
  "error": "DescripciÃ³n del error"
}
```

## ğŸ§ª Probar el Servicio

### Con cURL

```bash
# Health check
curl http://localhost:5000/health

# Analizar PDF
curl -X POST http://localhost:5000/analizar \
  -F "file=@/ruta/al/documento_tecnologico.pdf"
```

### Con Python (requests)

```python
import requests

# Analizar PDF
url = "http://localhost:5000/analizar"
files = {"file": open("documento_tecnologico.pdf", "rb")}
response = requests.post(url, files=files)
result = response.json()

if result["success"]:
    print("Informe TRL:")
    print(result["table_text"])
else:
    print("Error:", result["error"])
```

### Con Swagger UI

Abre en tu navegador:
```
http://localhost:5000/docs
```

## ğŸ” ConfiguraciÃ³n

El archivo `.env` contiene las siguientes variables:

```env
# Google Cloud Configuration
PROJECT_ID=tesis-475602
REGION=us-central1
MODEL_ID=gemini-2.0-pro

# API Configuration
API_HOST=0.0.0.0
API_PORT=5000
DEBUG=False

# CORS (ajusta segÃºn tu backend)
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8080
```

## ğŸ³ Docker

### Construir imagen

```bash
docker build -t microservicio-gemini .
```

### Ejecutar contenedor

```bash
docker run -p 5000:5000 \
  -v ~/.config/gcloud:/root/.config/gcloud:ro \
  --env-file .env \
  microservicio-gemini
```

### Con Docker Compose

```bash
# Iniciar
docker-compose up -d

# Ver logs
docker-compose logs -f

# Detener
docker-compose down
```

## ğŸš€ Despliegue en Google Cloud

### Cloud Run (Recomendado)

```bash
# 1. Configurar proyecto
gcloud config set project tesis-475602

# 2. Habilitar APIs necesarias
gcloud services enable run.googleapis.com
gcloud services enable cloudbuild.googleapis.com
gcloud services enable aiplatform.googleapis.com

# 3. Desplegar
gcloud run deploy microservicio-trl \
  --source . \
  --region us-central1 \
  --platform managed \
  --allow-unauthenticated \
  --port 5000 \
  --set-env-vars PROJECT_ID=tesis-475602,REGION=us-central1,MODEL_ID=gemini-2.0-flash
```

## ğŸ› ï¸ Desarrollo

### Agregar dependencias

```bash
pip install <paquete>
pip freeze > requirements.txt
```

### Ejecutar en modo desarrollo

```bash
# Con reload automÃ¡tico
uvicorn main:app --reload --port 5000
```

## ğŸ“ Notas Importantes

1. **Credenciales**: El servicio usa Application Default Credentials (ADC) de Google Cloud
2. **CORS**: Configura `ALLOWED_ORIGINS` en `.env` segÃºn los orÃ­genes de tu backend/frontend
3. **LÃ­mite de tamaÃ±o**: PDFs estÃ¡n limitados a 10MB por defecto
4. **Timeout**: Las peticiones a Gemini tienen timeout de 300 segundos (5 minutos)
5. **Modelo**: Usa `gemini-2.0-pro` para anÃ¡lisis TRL mÃ¡s precisos
6. **Matrices TRL**: Los archivos en `datTRL/` contienen las matrices oficiales de evaluaciÃ³n

## ğŸ› Troubleshooting

### Error de credenciales

```
Error obteniendo credenciales ADC
```

**SoluciÃ³n:**
```bash
gcloud auth application-default login
```

### Error de permisos

```
Vertex AI error 403
```

**SoluciÃ³n:** Verificar que tu cuenta tenga permisos para Vertex AI
```bash
gcloud projects add-iam-policy-binding tesis-475602 \
  --member="user:tu-email@gmail.com" \
  --role="roles/aiplatform.user"
```

### Puerto en uso

```
Address already in use
```

**SoluciÃ³n:** Cambiar el puerto en `.env` o detener el proceso que usa el puerto 5000

## ğŸ“„ Licencia

Este proyecto es parte de una tesis acadÃ©mica.

## ğŸ‘¤ Autor

Desarrollado para el proyecto de tesis TRL.
